{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I. Classification\n",
    "\n",
    "Dans une première partie, nous allons nous intéresser à la classification. On va importer des datasets de sklearn et entraîner notre modèle de XGBOOST sur de la classification binaire et multi-classes et le comparer avec deux modèles d'arbres de décision."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I.1 Importer les datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((455, 30), (114, 30))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris, fetch_covtype, load_breast_cancer  \n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Charger un jeu de données Iris pour classification\n",
    "\n",
    "cancer = load_breast_cancer()   # Pour la classification binaire\n",
    "X, Y = cancer.data, cancer.target\n",
    "\n",
    "# Diviser les données en ensembles d'entraînement et de test\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
    "\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I.2 Entraîner deux modèles d'arbres de décision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fin\n"
     ]
    }
   ],
   "source": [
    "from   sklearn.tree    import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from   sklearn.metrics import accuracy_score, f1_score\n",
    "from   sklearn         import tree\n",
    "import timeit\n",
    "\n",
    "Xchoix_train = X_train\n",
    "Ychoix_train = Y_train\n",
    "Xchoix_test  = X_test\n",
    "Ychoix_test  = Y_test\n",
    "# fnames       = iris.columns\n",
    "\n",
    "\n",
    "gini_stats          = []\n",
    "entropy_stats       = []\n",
    "randForest_stats       = []\n",
    "gini_classifieur    = DecisionTreeClassifier(criterion='gini'   , random_state=42)\n",
    "entropy_classifieur = DecisionTreeClassifier(criterion='entropy', random_state=42)\n",
    "\n",
    "# ============ GINI ====================\n",
    "# ............ Entraînement ............\n",
    "temps_debut = timeit.default_timer()\n",
    "gini_classifieur.fit(Xchoix_train, Ychoix_train)\n",
    "gini_stats.append(timeit.default_timer() - temps_debut)\n",
    "# ..... Evaluation entrainement ........\n",
    "Ychoix_pred = gini_classifieur.predict(Xchoix_train)\n",
    "gini_stats.append(accuracy_score(Ychoix_train, Ychoix_pred))\n",
    "gini_stats.append(f1_score(Ychoix_train, Ychoix_pred, average='micro'))\n",
    "# ................ Test ................\n",
    "temps_debut = timeit.default_timer()\n",
    "Ychoix_pred = gini_classifieur.predict(Xchoix_test)\n",
    "gini_stats.append(timeit.default_timer() - temps_debut)\n",
    "# ........... Evaluation test ...........\n",
    "gini_stats.append(accuracy_score(Ychoix_test, Ychoix_pred))\n",
    "gini_stats.append(f1_score(Ychoix_test, Ychoix_pred, average='micro'))\n",
    "\n",
    "# =========== Entropy ==================\n",
    "# ............ Entraînement ............\n",
    "temps_debut = timeit.default_timer()\n",
    "entropy_classifieur.fit(Xchoix_train, Ychoix_train)\n",
    "entropy_stats.append(timeit.default_timer() - temps_debut)\n",
    "# ..... Evaluation entrainement ........\n",
    "Ychoix_pred = entropy_classifieur.predict(Xchoix_train)\n",
    "entropy_stats.append(accuracy_score(Ychoix_train, Ychoix_pred))\n",
    "entropy_stats.append(f1_score(Ychoix_train, Ychoix_pred, average='micro'))\n",
    "# ................ Test ................\n",
    "temps_debut = timeit.default_timer()\n",
    "Ychoix_pred = entropy_classifieur.predict(Xchoix_test)\n",
    "entropy_stats.append(timeit.default_timer() - temps_debut)\n",
    "# ........... Evaluation test ...........\n",
    "entropy_stats.append(accuracy_score(Ychoix_test, Ychoix_pred))\n",
    "entropy_stats.append(f1_score(Ychoix_test, Ychoix_pred, average='micro'))\n",
    "\n",
    "# =========== Random forest ==================\n",
    "# ............ Entraînement ............\n",
    "temps_debut = timeit.default_timer()\n",
    "classifieur = RandomForestClassifier(n_estimators=100)\n",
    "classifieur.fit(Xchoix_train, Ychoix_train)\n",
    "randForest_stats.append(timeit.default_timer() - temps_debut)\n",
    "# ..... Evaluation entrainement ........\n",
    "Ychoix_pred = classifieur.predict(Xchoix_train)\n",
    "randForest_stats.append(accuracy_score(Ychoix_train, Ychoix_pred))\n",
    "randForest_stats.append(f1_score(Ychoix_train, Ychoix_pred, average='micro'))\n",
    "# ................ Test ................\n",
    "temps_debut = timeit.default_timer()\n",
    "Ychoix_pred = classifieur.predict(Xchoix_test)\n",
    "randForest_stats.append(timeit.default_timer() - temps_debut)\n",
    "# ........... Evaluation test ...........\n",
    "randForest_stats.append(accuracy_score(Ychoix_test, Ychoix_pred))\n",
    "randForest_stats.append(f1_score(Ychoix_test, Ychoix_pred, average='micro'))\n",
    "\n",
    "print('Fin')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I.3 Entraîner un modèle XGBOOST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "# Définir les paramètres du modèle\n",
    "# params = {\n",
    "#     'objective': 'multi:softmax',  # Fonction objective pour la classification multi-classe\n",
    "#     'num_class': 7,  # Nombre de classes dans le jeu de données \n",
    "#     'max_depth': 4  # Profondeur maximale de l'arbre\n",
    "# }\n",
    "# params = {\n",
    "#     'objective': 'binary:logistic',  # Fonction objective pour la classification binaire\n",
    "#     'num_class': 2, \n",
    "#     'max_depth': 3\n",
    "# }\n",
    "XGBOOST_stats = []\n",
    "\n",
    "# Créer le modèle de classification (avec les paramètres par défaut)\n",
    "clf = xgb.XGBClassifier()\n",
    "\n",
    "\n",
    "# ================= Entraînement =====================\n",
    "temps_debut = timeit.default_timer()\n",
    "# Entraîner le modèle\n",
    "clf.fit(X_train, Y_train)\n",
    "XGBOOST_stats.append(timeit.default_timer() - temps_debut)\n",
    "Y_pred = clf.predict(X_train)\n",
    "XGBOOST_stats.append(accuracy_score(Y_train, Y_pred))\n",
    "XGBOOST_stats.append(f1_score(Y_train, Y_pred, average='micro'))\n",
    "\n",
    "# ===================== Test ========================\n",
    "temps_debut = timeit.default_timer()\n",
    "# Faire des prédictions sur l'ensemble de test\n",
    "Y_pred = clf.predict(X_test)\n",
    "XGBOOST_stats.append(timeit.default_timer() - temps_debut)\n",
    "XGBOOST_stats.append(accuracy_score(Y_test, Y_pred))\n",
    "XGBOOST_stats.append(f1_score(Y_test, Y_pred, average='micro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Criteres</th>\n",
       "      <th>Entropie</th>\n",
       "      <th>Gini</th>\n",
       "      <th>XGBOOST</th>\n",
       "      <th>Random Forest</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Temps Entrainement</td>\n",
       "      <td>0.016999</td>\n",
       "      <td>0.018650</td>\n",
       "      <td>0.118002</td>\n",
       "      <td>0.485955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Accuracy Entrainement</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>F1 Entrainement</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Temps Test</td>\n",
       "      <td>0.000294</td>\n",
       "      <td>0.000323</td>\n",
       "      <td>0.001451</td>\n",
       "      <td>0.013052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Accuracy Test</td>\n",
       "      <td>0.947368</td>\n",
       "      <td>0.947368</td>\n",
       "      <td>0.956140</td>\n",
       "      <td>0.964912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>F1 Test</td>\n",
       "      <td>0.947368</td>\n",
       "      <td>0.947368</td>\n",
       "      <td>0.956140</td>\n",
       "      <td>0.964912</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                Criteres  Entropie      Gini   XGBOOST  Random Forest\n",
       "0     Temps Entrainement  0.016999  0.018650  0.118002       0.485955\n",
       "1  Accuracy Entrainement  1.000000  1.000000  1.000000       1.000000\n",
       "2        F1 Entrainement  1.000000  1.000000  1.000000       1.000000\n",
       "3             Temps Test  0.000294  0.000323  0.001451       0.013052\n",
       "4          Accuracy Test  0.947368  0.947368  0.956140       0.964912\n",
       "5                F1 Test  0.947368  0.947368  0.956140       0.964912"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame({\n",
    "    'Criteres' : ['Temps Entrainement', 'Accuracy Entrainement', 'F1 Entrainement', 'Temps Test', 'Accuracy Test', 'F1 Test'],\n",
    "    'Entropie' : entropy_stats,\n",
    "    'Gini'     : gini_stats,\n",
    "    'XGBOOST'  : XGBOOST_stats,\n",
    "    'Random Forest': randForest_stats\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# II. Régression\n",
    "\n",
    "Dans cette seconde partie, nous allons entraîner un modèle de XGBOOST pour la régression et le comparer avec le modèle de régression linéaire en utilisant une dataset de Sklearn comportant les prix des maisons de California."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### II.1 Importer la dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((16512, 8), (4128, 8))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "\n",
    "houses = fetch_california_housing()\n",
    "X, Y = houses.data, houses.target\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
    "\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### II.2 Entraîner un modèle de régression linéaire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 0.5558915986952461\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline      import make_pipeline\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "lineaire_stats = []\n",
    "\n",
    "reg_lineaire = make_pipeline(StandardScaler(with_mean=False), LinearRegression())\n",
    "\n",
    "temps_debut = timeit.default_timer()\n",
    "# Entraîner le modèle de régression linéaire\n",
    "reg_lineaire.fit(X_train, Y_train)\n",
    "lineaire_stats.append(timeit.default_timer() - temps_debut)\n",
    "\n",
    "temps_debut = timeit.default_timer()\n",
    "# Prédire\n",
    "Y_pred_lineaire = reg_lineaire.predict(X_test)\n",
    "lineaire_stats.append(timeit.default_timer() - temps_debut)\n",
    "\n",
    "# Calculer l'erreur quadratique moyenne (MSE)\n",
    "mse_lineaire = mean_squared_error(Y_test, Y_pred_lineaire)\n",
    "lineaire_stats.append(mse_lineaire)\n",
    "print(f\"MSE: {mse_lineaire}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### II.3 Entraîner un modèle XGBOOST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 0.2509968715591795\n"
     ]
    }
   ],
   "source": [
    "# Définir les paramètres du modèle\n",
    "params = {\n",
    "    'objective': 'reg:squarederror',  # Fonction objective pour la régression\n",
    "    'max_depth': 3  # Profondeur maximale de l'arbre\n",
    "}\n",
    "\n",
    "XGBOOST_stats = []\n",
    "\n",
    "# Créer le modèle de régression XGBOOST avec les paramètres, changer la profondeur change le MSE, plus profond mieux c'est mais prend plus de temps\n",
    "reg_XGBOOST = xgb.XGBRegressor(**params)\n",
    "\n",
    "temps_debut = timeit.default_timer()\n",
    "# Entraîner le modèle\n",
    "reg_XGBOOST.fit(X_train, Y_train)\n",
    "XGBOOST_stats.append(timeit.default_timer() - temps_debut)\n",
    "\n",
    "temps_debut = timeit.default_timer()\n",
    "# Faire des prédictions sur l'ensemble de test\n",
    "Y_pred_XGBOOST = reg_XGBOOST.predict(X_test)\n",
    "XGBOOST_stats.append(timeit.default_timer() - temps_debut)\n",
    "\n",
    "# Calculer l'erreur quadratique moyenne (MSE)\n",
    "mse_XGBOOST = mean_squared_error(Y_test, Y_pred_XGBOOST)\n",
    "XGBOOST_stats.append(mse_XGBOOST)\n",
    "print(f\"MSE: {mse_XGBOOST}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Criteres</th>\n",
       "      <th>Linéaire</th>\n",
       "      <th>XGBOOST</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Temps Entrainement</td>\n",
       "      <td>0.033214</td>\n",
       "      <td>0.152300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Temps Test</td>\n",
       "      <td>0.001046</td>\n",
       "      <td>0.003874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>MSE</td>\n",
       "      <td>0.555892</td>\n",
       "      <td>0.250997</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Criteres  Linéaire   XGBOOST\n",
       "0  Temps Entrainement  0.033214  0.152300\n",
       "1          Temps Test  0.001046  0.003874\n",
       "2                 MSE  0.555892  0.250997"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame({\n",
    "    'Criteres' : ['Temps Entrainement','Temps Test', 'MSE'],\n",
    "    'Linéaire' : lineaire_stats,\n",
    "    'XGBOOST'  : XGBOOST_stats\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "D85B-1HXegs5"
   },
   "source": [
    "## **IV. Exploration des Hyperparamètres**\n",
    "\n",
    "Dans cette section, nous examinerons plusieurs Hyperparamètres importants et comprendrons leurs fonctions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>is_canceled</th>\n",
       "      <th>lead_time</th>\n",
       "      <th>arrival_date_week_number</th>\n",
       "      <th>arrival_date_day_of_month</th>\n",
       "      <th>arrival_date_month</th>\n",
       "      <th>stays_in_weekend_nights</th>\n",
       "      <th>stays_in_week_nights</th>\n",
       "      <th>adults</th>\n",
       "      <th>children</th>\n",
       "      <th>babies</th>\n",
       "      <th>...</th>\n",
       "      <th>reserved_room_type_G</th>\n",
       "      <th>reserved_room_type_H</th>\n",
       "      <th>reserved_room_type_L</th>\n",
       "      <th>deposit_type_No_Deposit</th>\n",
       "      <th>deposit_type_Non_Refund</th>\n",
       "      <th>deposit_type_Refundable</th>\n",
       "      <th>customer_type_Contract</th>\n",
       "      <th>customer_type_Group</th>\n",
       "      <th>customer_type_Transient</th>\n",
       "      <th>customer_type_Transient-Party</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>342</td>\n",
       "      <td>27</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>737</td>\n",
       "      <td>27</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>27</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>27</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>14</td>\n",
       "      <td>27</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 53 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   is_canceled  lead_time  arrival_date_week_number  \\\n",
       "0            0        342                        27   \n",
       "1            0        737                        27   \n",
       "2            0          7                        27   \n",
       "3            0         13                        27   \n",
       "4            0         14                        27   \n",
       "\n",
       "   arrival_date_day_of_month  arrival_date_month  stays_in_weekend_nights  \\\n",
       "0                          1                   7                        0   \n",
       "1                          1                   7                        0   \n",
       "2                          1                   7                        0   \n",
       "3                          1                   7                        0   \n",
       "4                          1                   7                        0   \n",
       "\n",
       "   stays_in_week_nights  adults  children  babies  ...  reserved_room_type_G  \\\n",
       "0                     0       2       0.0       0  ...                     0   \n",
       "1                     0       2       0.0       0  ...                     0   \n",
       "2                     1       1       0.0       0  ...                     0   \n",
       "3                     1       1       0.0       0  ...                     0   \n",
       "4                     2       2       0.0       0  ...                     0   \n",
       "\n",
       "   reserved_room_type_H  reserved_room_type_L  deposit_type_No_Deposit  \\\n",
       "0                     0                     0                        1   \n",
       "1                     0                     0                        1   \n",
       "2                     0                     0                        1   \n",
       "3                     0                     0                        1   \n",
       "4                     0                     0                        1   \n",
       "\n",
       "   deposit_type_Non_Refund  deposit_type_Refundable  customer_type_Contract  \\\n",
       "0                        0                        0                       0   \n",
       "1                        0                        0                       0   \n",
       "2                        0                        0                       0   \n",
       "3                        0                        0                       0   \n",
       "4                        0                        0                       0   \n",
       "\n",
       "   customer_type_Group  customer_type_Transient  customer_type_Transient-Party  \n",
       "0                    0                        1                              0  \n",
       "1                    0                        1                              0  \n",
       "2                    0                        1                              0  \n",
       "3                    0                        1                              0  \n",
       "4                    0                        1                              0  \n",
       "\n",
       "[5 rows x 53 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bookings = pd.read_csv('data/hotel_bookings.csv')\n",
    "bookings.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define X and y\n",
    "X, y = bookings.iloc[:,1:], bookings.iloc[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'objective': 'binary:logistic',\n",
       " 'base_score': None,\n",
       " 'booster': None,\n",
       " 'callbacks': None,\n",
       " 'colsample_bylevel': None,\n",
       " 'colsample_bynode': None,\n",
       " 'colsample_bytree': None,\n",
       " 'device': None,\n",
       " 'early_stopping_rounds': None,\n",
       " 'enable_categorical': False,\n",
       " 'eval_metric': None,\n",
       " 'feature_types': None,\n",
       " 'gamma': None,\n",
       " 'grow_policy': None,\n",
       " 'importance_type': None,\n",
       " 'interaction_constraints': None,\n",
       " 'learning_rate': None,\n",
       " 'max_bin': None,\n",
       " 'max_cat_threshold': None,\n",
       " 'max_cat_to_onehot': None,\n",
       " 'max_delta_step': None,\n",
       " 'max_depth': None,\n",
       " 'max_leaves': None,\n",
       " 'min_child_weight': None,\n",
       " 'missing': nan,\n",
       " 'monotone_constraints': None,\n",
       " 'multi_strategy': None,\n",
       " 'n_estimators': None,\n",
       " 'n_jobs': None,\n",
       " 'num_parallel_tree': None,\n",
       " 'random_state': 123,\n",
       " 'reg_alpha': None,\n",
       " 'reg_lambda': None,\n",
       " 'sampling_method': None,\n",
       " 'scale_pos_weight': None,\n",
       " 'subsample': None,\n",
       " 'tree_method': None,\n",
       " 'validate_parameters': None,\n",
       " 'verbosity': None}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train and test split using sklearn\n",
    "X_train, X_test, y_train, y_test= train_test_split(X, y, test_size=.33, random_state=123)\n",
    "\n",
    "# Instatiate a XGBClassifier \n",
    "xgb_clf = xgb.XGBClassifier(random_state=123)\n",
    "\n",
    "# Inspect the parameters\n",
    "xgb_clf.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit it to the training set\n",
    "xgb_clf.fit(X_train, y_train)\n",
    "\n",
    "# Predict the labels of the test set\n",
    "preds = xgb_clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline accuracy: 0.8382308083375699\n"
     ]
    }
   ],
   "source": [
    "# Calculate the accuracy\n",
    "accuracy = float(np.sum(preds==y_test))/y_test.shape[0]\n",
    "\n",
    "# Print the baseline accuracy\n",
    "print(\"Baseline accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Instantiate the XGBClassifier with 25 boosting rounds\n",
    "xgb_clf = xgb.XGBClassifier(n_estimators=25, random_state=123)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yxd2UvBaegs-"
   },
   "source": [
    "### **Max depth**\n",
    "\n",
    "_Depuis la documentation de XGBoost :_\n",
    "> Profondeur maximale d'un arbre. Augmenter cette valeur rendra le modèle plus complexe et plus susceptible de surajuster.\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"https://github.com/datacamp/Machine-Learning-With-XGboost-live-training/blob/master/assets/max_depth.png?raw=true\" width = \"35%\"> \n",
    "</p>\n",
    "\n",
    "Voyons ce qui se passe lorsque nous augmentons la valeur de `max_depth` de 6 à 20.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.853329944077275"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set max_depth to 10\n",
    "xgb_clf.set_params(max_depth=20)\n",
    "\n",
    "# Fit the classifier to the training set\n",
    "xgb_clf.fit(X_train, y_train)\n",
    "\n",
    "# Predict the labels of the test set\n",
    "preds = xgb_clf.predict(X_test)\n",
    "\n",
    "# Compute the accuracy\n",
    "accuracy_score(y_test, preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "h3ug_cyUegtB"
   },
   "source": [
    "### **colsample_bytree**\n",
    "\n",
    "_Depuis la documentation de XGBoost :_\n",
    "\n",
    "> Le taux de sous-échantillonnage des colonnes lors de la construction de chaque arbre. Le sous-échantillonnage se produit une fois pour chaque arbre construit.\n",
    "\n",
    "Essentiellement, cela nous permet de limiter le nombre de colonnes utilisées lors de la construction de chaque arbre. Cela ajoute de l'aléatoire, rendant le modèle plus robuste au bruit. La valeur par défaut est 1 (c'est-à-dire toutes les colonnes), essayons une valeur plus petite.\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"https://github.com/datacamp/Machine-Learning-With-XGboost-live-training/blob/master/assets/colsample_bytree.gif?raw=true\" width = \"55%\"> \n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8552872394509405"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set colsample_bytree to 0.5 \n",
    "xgb_clf.set_params(colsample_bytree=0.5)\n",
    "\n",
    "# Fit the classifier to the training set\n",
    "xgb_clf.fit(X_train, y_train)\n",
    "\n",
    "# Predict the labels of the test set\n",
    "preds = xgb_clf.predict(X_test)\n",
    "\n",
    "# Compute the accuracy\n",
    "accuracy_score(y_test, preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dffKcMocegtE"
   },
   "source": [
    "Nous pouvons également limiter le nombre de colonnes utilisées à chaque niveau de profondeur ou nœud de notre arbre.\n",
    "\n",
    "_Selon la documentation de XGBoost :_\n",
    "\n",
    "> `colsample_bylevel` est le taux de sous-échantillonnage des colonnes pour chaque niveau. Le sous-échantillonnage se produit une fois pour chaque nouveau niveau de profondeur atteint dans un arbre. Les colonnes sont sous-échantillonnées à partir de l'ensemble des colonnes choisies pour l'arbre en cours.\n",
    "\n",
    "> `colsample_bynode` est le taux de sous-échantillonnage des colonnes pour chaque nœud (division). Le sous-échantillonnage se produit chaque fois qu'une nouvelle division est évaluée. Les colonnes sont sous-échantillonnées à partir de l'ensemble des colonnes choisies pour le niveau en cours."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Oc2NtL3WegtE"
   },
   "source": [
    "### **subsample**\n",
    "\n",
    "_Selon la documentation de XGBoost :_\n",
    "\n",
    "> - Taux de sous-échantillonnage des instances d'entraînement. Le régler sur 0.5 signifie que XGBoost échantillonnera de manière aléatoire la moitié des données d'entraînement avant de développer les arbres, ce qui permettra d'éviter le surajustement. \n",
    "> - Le sous-échantillonnage se produira une fois à chaque itération.\n",
    "> - range: (0,1]\n",
    "\n",
    "La valeur par défaut est 1, essayons 0.75.\n",
    "\n",
    "Cela signifie que chacun de nos 25 arbres recevra un échantillonnage aléatoire de 75 % de nos données d'entraînement. Chaque arbre s'entraînera sur différentes parties des données, ce qui ajoute de l'aléatoire (similaire à `colsample_bytree`). \n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"https://github.com/datacamp/Machine-Learning-With-XGboost-live-training/blob/master/assets/subsample.gif?raw=true\" width = \"55%\"> \n",
    "</p>\n",
    "\n",
    "Cependant, nous ne voulons pas que cette valeur soit trop basse si nous n'avons pas beaucoup d'arbres, car notre modèle risquerait de ne pas voir suffisamment de données et de ne pas s'ajuster correctement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8501016776817488"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set subsample to 0.75 \n",
    "xgb_clf.set_params(subsample=0.75)\n",
    "\n",
    "# Fit the classifier to the training set\n",
    "xgb_clf.fit(X_train, y_train)\n",
    "\n",
    "# Predict the labels of the test set\n",
    "preds = xgb_clf.predict(X_test)\n",
    "\n",
    "# Compute the accuracy\n",
    "accuracy_score(y_test, preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9TQEfG-hegtK"
   },
   "source": [
    "### **gamma**\n",
    "\n",
    "_Selon la documentation de XGBoost :_\n",
    "> - Perte minimale requise pour effectuer une partition supplémentaire sur un nœud feuille de l'arbre. Plus gamma est grand, plus l'algorithme sera conservateur.\n",
    "> - range: [0,∞]\n",
    "\n",
    "\n",
    "Cela décide si un nœud se scindera en fonction de la réduction de perte attendue après la division. `gamma` représente la perte minimale requise pour qu'un nœud se scinde.\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"https://github.com/datacamp/Machine-Learning-With-XGboost-live-training/blob/master/assets/gamma.png?raw=true\" width = \"55%\"> \n",
    "</p>\n",
    "\n",
    "Augmenter `gamma` = moins de divisions = moins de complexité\n",
    "\n",
    "La valeur par défaut est 0, donc dans notre cas, les nœuds se sont toujours divisés jusqu'à la profondeur maximale. Augmentons cela à 0.25."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.850127097102186"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set gamma to .25 \n",
    "xgb_clf.set_params(gamma=0.25)\n",
    "\n",
    "# Fit the classifier to the training set\n",
    "xgb_clf.fit(X_train, y_train)\n",
    "\n",
    "# Predict the labels of the test set\n",
    "preds = xgb_clf.predict(X_test)\n",
    "\n",
    "# Compute the accuracy\n",
    "accuracy_score(y_test, preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Tp92TsxLegtQ"
   },
   "source": [
    "### **Learning Rate (eta)**\n",
    "\n",
    "_Selon la documentation de XGBoost :_\n",
    "> - Taux de réduction utilisé dans la mise à jour pour prévenir le surajustement. Après chaque étape d'amplification, nous pouvons obtenir directement les poids des nouvelles fonctionnalités, et eta réduit les poids des fonctionnalités pour rendre le processus de boosting plus conservateur.\n",
    "> - range: [0,1]\n",
    "\n",
    "Le taux d'apprentissage affecte la rapidité avec laquelle un modèle apprend.\n",
    "\n",
    "Le boosting gradient fonctionne en ajoutant séquentiellement des faibles apprenants au modèle. Chaque nouveau faible apprenant tente de corriger les erreurs résiduelles des arbres précédents. Cela rend le modèle très susceptible de surajustement. Le taux d'apprentissage peut aider à ralentir l'apprentissage en réduisant les poids résultants de l'arbre actuel avant de les transmettre à l'arbre suivant.\n",
    "\n",
    "Le taux d'apprentissage actuel de notre modèle est de 0.1. Que se passe-t-il si nous le changeons à 0.3 ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.850127097102186"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set learning rate to .3 \n",
    "xgb_clf.set_params(learning_rate=0.3)\n",
    "\n",
    "# Fit the classifier to the training set\n",
    "xgb_clf.fit(X_train, y_train)\n",
    "\n",
    "# Predict the labels of the test set\n",
    "preds = xgb_clf.predict(X_test)\n",
    "\n",
    "# Compute the accuracy\n",
    "accuracy_score(y_test, preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qIQ9UJzjDERA"
   },
   "source": [
    "Le taux d'apprentissage et le nombre d'arbres doivent être ajustés ensemble. Si nous diminuons le taux d'apprentissage, nous devons nous assurer d'avoir suffisamment d'arbres pour apprendre quelque chose et éviter un sous-ajustement sévère. **Par conséquent, un faible taux d'apprentissage nécessitera davantage de cycles de boosting.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "afmWgrWCegtM"
   },
   "source": [
    "### **reg_alpha**\n",
    "\n",
    "_Selon la documentation de XGBoost :_\n",
    "> Terme de régularisation L1 sur les poids. Augmenter cette valeur rendra le modèle plus conservateur.\n",
    "\n",
    "L1 est souvent appelé **régression Lasso**. Il s'agit d'une technique de régularisation fondamentale, ce qui signifie qu'elle vise à réduire le surajustement en décourageant les modèles complexes. Dans le cas du boosting gradient, L1 le fait en ajoutant des pénalités sur les poids des feuilles. Augmenter alpha conduit les poids des feuilles des apprenants de base vers 0.\n",
    "\n",
    "La valeur par défaut est 0, ce qui signifie qu'il n'y a actuellement aucune régularisation alpha dans notre modèle. Activons L1 avec une valeur de `0.01`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.850127097102186"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# set reg_alpha to .1 \n",
    "xgb_clf.set_params(reg_alpha=0.01)\n",
    "\n",
    "# Fit the classifier to the training set\n",
    "xgb_clf.fit(X_train, y_train)\n",
    "\n",
    "# Predict the labels of the test set\n",
    "preds = xgb_clf.predict(X_test)\n",
    "\n",
    "# Compute the accuracy\n",
    "accuracy_score(y_test, preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SDd4PjwCegtP"
   },
   "source": [
    "**L2**, également connue sous le nom de régression ridge, est également disponible avec le paramètre reg_lambda. L2 est réputée pour avoir une pénalité plus douce que L1. Cela signifie que les poids des feuilles diminuent de manière plus régulière, avec moins de risque de parcimonie dans les poids des feuilles. Assurez-vous donc d'essayer différentes techniques de régularisation !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AJUwZYohz4Wu"
   },
   "source": [
    "Examinons les paramètres résultants après les avoir modifiés manuellement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'objective': 'binary:logistic',\n",
       " 'base_score': None,\n",
       " 'booster': None,\n",
       " 'callbacks': None,\n",
       " 'colsample_bylevel': None,\n",
       " 'colsample_bynode': None,\n",
       " 'colsample_bytree': 0.5,\n",
       " 'device': None,\n",
       " 'early_stopping_rounds': None,\n",
       " 'enable_categorical': False,\n",
       " 'eval_metric': None,\n",
       " 'feature_types': None,\n",
       " 'gamma': 0.25,\n",
       " 'grow_policy': None,\n",
       " 'importance_type': None,\n",
       " 'interaction_constraints': None,\n",
       " 'learning_rate': 0.3,\n",
       " 'max_bin': None,\n",
       " 'max_cat_threshold': None,\n",
       " 'max_cat_to_onehot': None,\n",
       " 'max_delta_step': None,\n",
       " 'max_depth': 20,\n",
       " 'max_leaves': None,\n",
       " 'min_child_weight': None,\n",
       " 'missing': nan,\n",
       " 'monotone_constraints': None,\n",
       " 'multi_strategy': None,\n",
       " 'n_estimators': 25,\n",
       " 'n_jobs': None,\n",
       " 'num_parallel_tree': None,\n",
       " 'random_state': 123,\n",
       " 'reg_alpha': 0.01,\n",
       " 'reg_lambda': None,\n",
       " 'sampling_method': None,\n",
       " 'scale_pos_weight': None,\n",
       " 'subsample': 0.75,\n",
       " 'tree_method': None,\n",
       " 'validate_parameters': None,\n",
       " 'verbosity': None}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print the model parameters\n",
    "xgb_clf.get_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Bc-k9UF1egtY"
   },
   "source": [
    "Il existe de nombreuses combinaisons possibles de paramètres. Nous ne pouvons pas les ajuster manuellement et les choisir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
